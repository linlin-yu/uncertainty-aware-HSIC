{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import load_ood_data, compute_kde\n",
    "from utils.yaml import read_yaml_file\n",
    "import torch\n",
    "from scipy import io as scio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "dataset = 'Houston'\n",
    "model = 'TLRSR'\n",
    "root = os.path.join('anomaly_detection', 'code-for-PCA-TLRSR')\n",
    "files = os.listdir(os.path.join(root,f'{dataset}_result'))\n",
    "ood_list = [0]\n",
    "anomaly_score = scio.loadmat(os.path.join(root, f'{dataset}_result', files[0]))['anomaly_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Organize result for TLRSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'PaviaU'\n",
    "model = 'TLRSR'\n",
    "root = os.path.join('anomaly_detection', 'code-for-PCA-TLRSR')\n",
    "files = os.listdir(os.path.join(root,f'{dataset}_result'))\n",
    "# ood_list = [0]\n",
    "ood_list = [0,1,2,3,4,5,6,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ood in ood_list:\n",
    "    config = read_yaml_file(path='', directory='configs', file_name=f'ood_config_{dataset}')\n",
    "    config['data']['ood_left_out_classes'] = [ood,]\n",
    "    data, num_classes = load_ood_data(config['data'])\n",
    "    for file in files:\n",
    "        result = {}\n",
    "        result['model'] = 'TLRSR'\n",
    "        result['dataset'] = dataset\n",
    "        result['ood'] = ood\n",
    "        name = file.strip('.mat').split('_')\n",
    "        result['lambda_1'] = name[-2]\n",
    "        result['lambda_2'] = name[-1]\n",
    "        # anomaly_score\n",
    "        # print(file)\n",
    "        anomaly_score = scio.loadmat(os.path.join(root, f'{dataset}_result', file))['anomaly_score']\n",
    "        anomaly_score_1D = np.float32(anomaly_score.reshape(-1,))\n",
    "        # get the roc and pr\n",
    "        anomaly_score_1D_selected = anomaly_score_1D[data.labeled_indices]\n",
    "        mask = data.ood_test_mask |data.id_test_mask\n",
    "        corrects = data.ood_mask\n",
    "        fpr, tpr, _ = metrics.roc_curve(corrects[mask], anomaly_score_1D_selected[mask])\n",
    "        roc = metrics.auc(fpr, tpr)\n",
    "        prec, rec, _ = metrics.precision_recall_curve(corrects[mask], anomaly_score_1D_selected[mask])\n",
    "        pr = metrics.auc(rec, prec)\n",
    "        result['test_id_oa_mean'] = 'n.a.'\n",
    "        result['test_id_oa_std'] = 'n.a'\n",
    "        result['test_roc_mean'] = roc\n",
    "        result['test_roc_std'] = 'n.a'\n",
    "        result['test_pr_mean'] = pr\n",
    "        result['test_pr_std'] = 'n.a.'\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv(f'./results/TLRST_{model}_{dataset}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. organize the result for RGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Houston'\n",
    "model = 'RGAE'\n",
    "root = os.path.join('anomaly_detection', 'Hyperspectral-anomaly-detection-with-RGAE-main')\n",
    "files = os.listdir(os.path.join(root,f'{dataset}_result'))\n",
    "# ood_list = [0]\n",
    "ood_list = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ood in ood_list:\n",
    "    config = read_yaml_file(path='', directory='configs', file_name=f'ood_config_{dataset}')\n",
    "    config['data']['ood_left_out_classes'] = [ood,]\n",
    "    data, num_classes = load_ood_data(config['data'])\n",
    "    for file in files:\n",
    "        result = {}\n",
    "        result['model'] = 'RGAE'\n",
    "        result['dataset'] = dataset\n",
    "        result['ood'] = ood\n",
    "        name = file.strip('.mat').split('_')\n",
    "        result['lambda'] = name[-3]\n",
    "        result['S'] = name[-2]\n",
    "        result['n_hid'] = name[-1]\n",
    "        # anomaly_score\n",
    "        # print(file)\n",
    "        anomaly_score = scio.loadmat(os.path.join(root, f'{dataset}_result', file))['y']\n",
    "        anomaly_score_1D = np.float32(anomaly_score.reshape(-1,))\n",
    "        # get the roc and pr\n",
    "        anomaly_score_1D_selected = anomaly_score_1D[data.labeled_indices]\n",
    "        mask = data.ood_test_mask |data.id_test_mask\n",
    "        corrects = data.ood_mask\n",
    "        fpr, tpr, _ = metrics.roc_curve(corrects[mask], anomaly_score_1D_selected[mask])\n",
    "        roc = metrics.auc(fpr, tpr)\n",
    "        prec, rec, _ = metrics.precision_recall_curve(corrects[mask], anomaly_score_1D_selected[mask])\n",
    "        pr = metrics.auc(rec, prec)\n",
    "        result['test_id_oa_mean'] = 'n.a.'\n",
    "        result['test_id_oa_std'] = 'n.a'\n",
    "        result['test_roc_mean'] = roc\n",
    "        result['test_roc_std'] = 'n.a'\n",
    "        result['test_pr_mean'] = pr\n",
    "        result['test_pr_std'] = 'n.a.'\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv(f'./results/ad_{model}_{dataset}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot roc and pr figure\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import auc\n",
    "def plot_roc(fpr, tpr):\n",
    "\n",
    "    fig = px.area(\n",
    "        x=fpr, y=tpr,\n",
    "        title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n",
    "        labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "        width=700, height=500\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type='line', line=dict(dash='dash'),\n",
    "        x0=0, x1=1, y0=0, y1=1\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "    fig.update_xaxes(constrain='domain')\n",
    "    return fig\n",
    "\n",
    "def plot_pr(fpr, tpr, precision, recall):\n",
    "    fig = px.area(\n",
    "        x=recall, y=precision,\n",
    "        title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n",
    "        labels=dict(x='Recall', y='Precision'),\n",
    "        width=700, height=500\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type='line', line=dict(dash='dash'),\n",
    "        x0=0, x1=1, y0=1, y1=0\n",
    "    )\n",
    "    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "    fig.update_xaxes(constrain='domain')\n",
    "\n",
    "    return fig\n",
    "fig_roc = plot_roc(fpr, tpr)\n",
    "fig_pr = plot_pr(fpr, tpr, prec, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_roc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pr.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
